{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Trading System - Interactive Development Pipeline\n",
    "\n",
    "This notebook walks through the entire ML trading pipeline step-by-step.\n",
    "\n",
    "**Purpose**: Learn and experiment with each component during development phase.\n",
    "\n",
    "**Steps**:\n",
    "1. Data Ingestion (~5 min)\n",
    "2. Feature Engineering (~2 min)\n",
    "3. Model Training (~5 min)\n",
    "4. Backtesting (~2 min)\n",
    "5. Dashboard Population (~1 min)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = os.path.join(project_root, 'config/config.yaml')\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  - Modeling algorithm: {config['modeling']['algorithm']}\")\n",
    "print(f\"  - Portfolio top_k: {config['portfolio']['top_k']}\")\n",
    "print(f\"  - Optimizer: {config['portfolio']['optimizer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Ingestion\n",
    "\n",
    "Fetch OHLCV data for a subset of S&P 500 stocks.\n",
    "\n",
    "**Tip**: Start with 20-30 stocks for quick testing, expand later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.io.universe import load_sp500_constituents\n",
    "from src.io.ingest_ohlcv import OHLCVIngester\n",
    "\n",
    "# Load universe - START SMALL for testing\n",
    "NUM_STOCKS = 30  # Adjust this: 20-30 for testing, 100+ for production\n",
    "symbols = load_sp500_constituents()[:NUM_STOCKS]\n",
    "\n",
    "print(f\"Selected {len(symbols)} stocks for analysis\")\n",
    "print(f\"Sample symbols: {symbols[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch OHLCV data from yfinance\n",
    "ingester = OHLCVIngester()\n",
    "start_date = config['ingest']['start_date']  # From config.yaml\n",
    "\n",
    "print(f\"Fetching data from {start_date} to present...\")\n",
    "print(\"This may take 2-5 minutes depending on number of stocks...\\n\")\n",
    "\n",
    "data = ingester.fetch_ohlcv(symbols, start_date, None)\n",
    "\n",
    "print(f\"\\n✓ Fetched data for {len(data)} symbols\")\n",
    "print(f\"  Total rows: {sum(len(df) for df in data.values()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet format\n",
    "ingester.save_parquet(data)\n",
    "print(\"✓ Data saved to data/parquet/\")\n",
    "\n",
    "# Explore the data\n",
    "sample_symbol = symbols[0]\n",
    "sample_df = data[sample_symbol]\n",
    "\n",
    "print(f\"\\nSample data for {sample_symbol}:\")\n",
    "print(sample_df.head())\n",
    "print(f\"\\nDate range: {sample_df['date'].min()} to {sample_df['date'].max()}\")\n",
    "print(f\"Total trading days: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample stock price\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Price chart\n",
    "axes[0].plot(sample_df['date'], sample_df['close'], linewidth=1.5)\n",
    "axes[0].set_title(f\"{sample_symbol} - Closing Price\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume chart\n",
    "axes[1].bar(sample_df['date'], sample_df['volume'], alpha=0.6)\n",
    "axes[1].set_title(f\"{sample_symbol} - Trading Volume\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Volume')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.io.ingest_fundamentals import FundamentalsIngester\n",
    "\n",
    "# Initialize fundamental data ingester\n",
    "fund_ingester = FundamentalsIngester(storage_path=\"data/fundamentals\")\n",
    "\n",
    "print(f\"Fetching fundamental data for {len(symbols)} symbols...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Fetch fundamental data\n",
    "fundamental_data = fund_ingester.fetch_fundamentals(symbols)\n",
    "\n",
    "print(f\"\\n✓ Fetched fundamental data for {len(fundamental_data)} symbols\")\n",
    "\n",
    "# Save to parquet\n",
    "fund_ingester.save_parquet(fundamental_data)\n",
    "print(\"✓ Fundamental data saved to data/fundamentals/\")\n",
    "\n",
    "# Preview sample data\n",
    "if len(fundamental_data) > 0:\n",
    "    sample_symbol = list(fundamental_data.keys())[0]\n",
    "    sample_fund = fundamental_data[sample_symbol]\n",
    "    print(f\"\\nSample fundamental data for {sample_symbol}:\")\n",
    "    print(sample_fund.head())\n",
    "    print(f\"\\nColumns: {list(sample_fund.columns)}\")\n",
    "    print(f\"Date range: {sample_fund['date'].min()} to {sample_fund['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Fetch Fundamental Data\n",
    "\n",
    "Now let's fetch fundamental data (income statements, balance sheets, cash flow statements) for the same symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Feature Engineering\n",
    "\n",
    "Generate technical indicators and forward return labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.ta_features import create_technical_features\n",
    "from src.labeling.labels import generate_forward_returns\n",
    "\n",
    "# Load saved parquet data\n",
    "df = ingester.load_parquet(symbols)\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows of OHLCV data\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Unique symbols: {df['symbol'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5: Add Fundamental Features with PIT Alignment\n",
    "\n",
    "Now let's integrate fundamental features using Point-in-Time (PIT) alignment to prevent look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the module to get latest fixes\n",
    "import importlib\n",
    "import sys\n",
    "if 'src.features.fa_features' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.features.fa_features'])\n",
    "    print(\"✓ Reloaded fa_features module\")\n",
    "\n",
    "from src.features.fa_features import FundamentalFeatures\n",
    "\n",
    "print(\"Loading fundamental data...\")\n",
    "# Load all saved fundamental data\n",
    "fundamentals_df = fund_ingester.load_parquet(symbols)\n",
    "\n",
    "if fundamentals_df.empty:\n",
    "    print(\"⚠️ No fundamental data found. Run Step 1.5 first.\")\n",
    "else:\n",
    "    print(f\"✓ Loaded {len(fundamentals_df)} fundamental records\")\n",
    "    print(f\"  Symbols: {fundamentals_df['symbol'].nunique()}\")\n",
    "    print(f\"  Date range: {fundamentals_df['date'].min()} to {fundamentals_df['date'].max()}\")\n",
    "    \n",
    "    # Initialize fundamental features with PIT alignment\n",
    "    fa_features = FundamentalFeatures(config)\n",
    "    \n",
    "    print(\"\\nComputing fundamental features with PIT alignment...\")\n",
    "    print(f\"  PIT constraints:\")\n",
    "    print(f\"    - Min lag: {fa_features.pit_min_lag_days} days\")\n",
    "    print(f\"    - Default publication lag: {fa_features.default_public_lag_days} days\")\n",
    "    print(f\"    - Earnings blackout: {fa_features.earnings_blackout_days} days\")\n",
    "    \n",
    "    # Store original column count\n",
    "    original_cols = len(df.columns)\n",
    "    \n",
    "    # Compute and align fundamental features\n",
    "    df = fa_features.compute_features(df, fundamentals_df)\n",
    "    \n",
    "    new_cols = len(df.columns) - original_cols\n",
    "    print(f\"\\n✓ Fundamental features added: {new_cols} new columns\")\n",
    "    \n",
    "    # Show new fundamental feature columns\n",
    "    fund_feature_cols = [col for col in df.columns if any(x in col for x in ['ratio', 'roe', 'roa', 'debt', 'margin', 'qoq', 'quality', 'equity', 'revenue', 'income', 'cash_flow'])]\n",
    "    print(f\"\\nSample fundamental features:\")\n",
    "    for col in fund_feature_cols[:15]:\n",
    "        print(f\"  - {col}\")\n",
    "    if len(fund_feature_cols) > 15:\n",
    "        print(f\"  ... and {len(fund_feature_cols) - 15} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate technical features\n",
    "print(\"Generating technical features...\")\n",
    "df = create_technical_features(df, config)\n",
    "\n",
    "print(f\"\\n✓ Technical features created\")\n",
    "print(f\"  Total columns now: {len(df.columns)}\")\n",
    "\n",
    "# Show feature columns\n",
    "feature_cols = [col for col in df.columns if col not in ['date', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'adj_close']]\n",
    "print(f\"\\nGenerated features ({len(feature_cols)}):\")\n",
    "for col in feature_cols[:10]:\n",
    "    print(f\"  - {col}\")\n",
    "if len(feature_cols) > 10:\n",
    "    print(f\"  ... and {len(feature_cols) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forward return labels\n",
    "print(\"Generating forward return labels...\")\n",
    "df = generate_forward_returns(df, config)\n",
    "\n",
    "label_col = f\"forward_return_{config['labels']['horizon']}d\"\n",
    "print(f\"\\n✓ Labels created: {label_col}\")\n",
    "print(f\"\\nLabel statistics:\")\n",
    "print(df[label_col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df[label_col].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Forward Returns Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(f'{label_col} (%)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero return')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot by symbol (sample)\n",
    "sample_symbols = symbols[:10]\n",
    "sample_data = df[df['symbol'].isin(sample_symbols)]\n",
    "sample_data.boxplot(column=label_col, by='symbol', ax=axes[1], rot=45)\n",
    "axes[1].set_title('Forward Returns by Symbol (Sample)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Symbol')\n",
    "axes[1].set_ylabel(f'{label_col} (%)')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features\n",
    "from src.io.storage import save_dataframe\n",
    "\n",
    "save_dataframe(df, 'data/features/all_features.parquet')\n",
    "print(\"✓ Features saved to data/features/all_features.parquet\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(f\"\\nMissing values (top 10 columns with most nulls):\")\n",
    "print(missing_pct.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fundamental features distribution\n",
    "try:\n",
    "    fundamentals_df\n",
    "    fund_feature_cols\n",
    "    can_visualize = not fundamentals_df.empty and len(fund_feature_cols) > 0\n",
    "except NameError:\n",
    "    can_visualize = False\n",
    "    print(\"ℹ️ No fundamental features to visualize\")\n",
    "\n",
    "if can_visualize:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Select key fundamental features\n",
    "    viz_features = []\n",
    "    for feat in ['pe_ratio_calc', 'roe_calc', 'debt_to_equity_calc', 'profit_margin']:\n",
    "        if feat in df.columns:\n",
    "            viz_features.append(feat)\n",
    "    \n",
    "    if len(viz_features) >= 4:\n",
    "        for idx, feat in enumerate(viz_features[:4]):\n",
    "            row = idx // 2\n",
    "            col = idx % 2\n",
    "            \n",
    "            data = df[feat].dropna()\n",
    "            if len(data) > 0:\n",
    "                axes[row, col].hist(data, bins=50, alpha=0.7, edgecolor='black')\n",
    "                axes[row, col].set_title(f'{feat} Distribution', fontsize=12, fontweight='bold')\n",
    "                axes[row, col].set_xlabel(feat)\n",
    "                axes[row, col].set_ylabel('Frequency')\n",
    "                axes[row, col].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add mean/median lines\n",
    "                mean_val = data.mean()\n",
    "                median_val = data.median()\n",
    "                axes[row, col].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "                axes[row, col].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "                axes[row, col].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FUNDAMENTAL FEATURES SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total fundamental features: {len(fund_feature_cols)}\")\n",
    "        print(f\"\\nCoverage (non-null ratio):\")\n",
    "        for feat in fund_feature_cols[:10]:\n",
    "            if feat in df.columns:\n",
    "                coverage = (df[feat].notna().sum() / len(df)) * 100\n",
    "                print(f\"  {feat}: {coverage:.1f}%\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(f\"⚠️ Only {len(viz_features)} fundamental features found (need 4 for visualization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Fundamental Features Distribution\n",
    "\n",
    "Visualize the distribution of key fundamental features to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Check PIT alignment\n",
    "try:\n",
    "    fundamentals_df\n",
    "    has_fundamentals = not fundamentals_df.empty\n",
    "except NameError:\n",
    "    has_fundamentals = False\n",
    "    print(\"⚠️ fundamentals_df not defined - skipping PIT verification\")\n",
    "\n",
    "if has_fundamentals and 'public_date' in df.columns:\n",
    "    print(\"=\"*70)\n",
    "    print(\"PIT ALIGNMENT VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check 1: Ensure all public_dates are before or equal to price dates\n",
    "    df_with_fund = df[df['public_date'].notna()].copy()\n",
    "    \n",
    "    if len(df_with_fund) > 0:\n",
    "        # Calculate days between public_date and price date\n",
    "        df_with_fund['days_after_publication'] = (df_with_fund['date'] - df_with_fund['public_date']).dt.days\n",
    "        \n",
    "        print(f\"\\n✓ Fundamental data coverage: {len(df_with_fund):,} rows ({len(df_with_fund)/len(df)*100:.1f}%)\")\n",
    "        print(f\"\\nDays between publication and price date:\")\n",
    "        print(df_with_fund['days_after_publication'].describe())\n",
    "        \n",
    "        # Check for any violations (price date before public date)\n",
    "        violations = df_with_fund[df_with_fund['days_after_publication'] < 0]\n",
    "        if len(violations) > 0:\n",
    "            print(f\"\\n⚠️ WARNING: Found {len(violations)} PIT violations!\")\n",
    "            print(\"These rows have price dates BEFORE fundamental publication dates.\")\n",
    "            print(violations[['date', 'symbol', 'public_date', 'days_after_publication']].head())\n",
    "        else:\n",
    "            print(f\"\\n✓ No PIT violations detected\")\n",
    "        \n",
    "        # Check 2: Verify minimum lag constraint\n",
    "        try:\n",
    "            fa_features\n",
    "            min_lag_violations = df_with_fund[df_with_fund['days_after_publication'] < fa_features.pit_min_lag_days]\n",
    "            if len(min_lag_violations) > 0:\n",
    "                print(f\"\\n⚠️ WARNING: {len(min_lag_violations)} rows violate minimum lag ({fa_features.pit_min_lag_days} days)\")\n",
    "            else:\n",
    "                print(f\"\\n✓ All rows respect minimum lag constraint ({fa_features.pit_min_lag_days} days)\")\n",
    "        except NameError:\n",
    "            pass\n",
    "        \n",
    "        # Sample verification\n",
    "        print(f\"\\nSample PIT-aligned data (5 random rows):\")\n",
    "        sample_cols = ['date', 'symbol', 'public_date', 'days_after_publication']\n",
    "        if 'pe_ratio_calc' in df_with_fund.columns:\n",
    "            sample_cols.append('pe_ratio_calc')\n",
    "        \n",
    "        sample_rows = df_with_fund.sample(min(5, len(df_with_fund)))[sample_cols]\n",
    "        for _, row in sample_rows.iterrows():\n",
    "            print(f\"  {row['symbol']} on {row['date'].date()}: data from {row['public_date'].date()} ({int(row['days_after_publication'])} days ago)\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "elif has_fundamentals:\n",
    "    print(\"⚠️ Cannot verify PIT alignment - public_date column not found\")\n",
    "else:\n",
    "    print(\"ℹ️ No fundamental data loaded - PIT verification skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification: PIT Alignment Check\n",
    "\n",
    "Verify that fundamental data is properly aligned and no look-ahead bias exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Model Training\n",
    "\n",
    "Train ML model to predict forward returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.dataset import MLDataset, create_time_based_split\n",
    "from src.ml.train import ModelTrainer\n",
    "\n",
    "# Prepare dataset\n",
    "label_col = f\"forward_return_{config['labels']['horizon']}d\"\n",
    "dataset = MLDataset(label_col=label_col)\n",
    "\n",
    "print(f\"Preparing ML dataset with label: {label_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based train/test split (crucial for time-series)\n",
    "embargo_days = config['modeling'].get('embargo_days', 5)\n",
    "train_df, test_df = create_time_based_split(df, test_size=0.2, embargo_days=embargo_days)\n",
    "\n",
    "print(f\"✓ Data split with {embargo_days}-day embargo\")\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  Rows: {len(train_df):,}\")\n",
    "print(f\"  Date range: {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Rows: {len(test_df):,}\")\n",
    "print(f\"  Date range: {test_df['date'].min()} to {test_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with MLflow tracking\n",
    "trainer = ModelTrainer(config)\n",
    "\n",
    "print(f\"Training {config['modeling']['algorithm']} model...\")\n",
    "print(\"This may take 3-5 minutes...\\n\")\n",
    "\n",
    "trainer.train_with_mlflow(\n",
    "    X_train, y_train, \n",
    "    X_test, y_test, \n",
    "    run_name=\"interactive_notebook_run\"\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X_train, y_train = dataset.prepare(train_df, auto_select_features=True)\n",
    "X_test, y_test = dataset.prepare(test_df, auto_select_features=False)\n",
    "\n",
    "print(f\"✓ Features prepared\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Samples: {len(X_train):,}\")\n",
    "print(f\"  Features: {len(X_train.columns)}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Samples: {len(X_test):,}\")\n",
    "print(f\"\\nSelected features:\")\n",
    "for feat in X_train.columns[:15]:\n",
    "    print(f\"  - {feat}\")\n",
    "if len(X_train.columns) > 15:\n",
    "    print(f\"  ... and {len(X_train.columns) - 15} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "metrics = trainer.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\n=\" * 50)\n",
    "print(\"MODEL EVALUATION METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nInformation Coefficient (IC):      {metrics['ic']:.4f}\")\n",
    "print(f\"Rank IC:                            {metrics['rank_ic']:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE):           {metrics['mse']:.6f}\")\n",
    "print(f\"R² Score:                           {metrics.get('r2', 0):.4f}\")\n",
    "\n",
    "# IC interpretation guide\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"IC Interpretation Guide:\")\n",
    "print(\"  |IC| > 0.05: Good predictive power\")\n",
    "print(\"  |IC| > 0.10: Excellent predictive power\")\n",
    "print(\"  IC > 0:     Positive correlation (correct direction)\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actuals\n",
    "predictions = trainer.predict(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, predictions, alpha=0.3, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Forward Returns')\n",
    "axes[0].set_ylabel('Predicted Forward Returns')\n",
    "axes[0].set_title('Predictions vs Actuals', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - predictions\n",
    "axes[1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero error')\n",
    "axes[1].set_xlabel('Prediction Error')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Prediction Residuals', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import os\n",
    "model_path = 'data/models/latest_model.pkl'\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "trainer.save_model(model_path)\n",
    "print(f\"✓ Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Portfolio Construction & Backtesting\n",
    "\n",
    "Use ML scores to construct portfolio and backtest with realistic costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.portfolio.construct import construct_portfolio\n",
    "from src.backtest.bt_engine import VectorizedBacktester\n",
    "\n",
    "# Generate scores for test period\n",
    "X_test_full, _ = dataset.prepare(test_df, auto_select_features=False)\n",
    "scores = trainer.predict(X_test_full)\n",
    "\n",
    "# Filter test_df to match X_test_full (remove rows with missing labels)\n",
    "test_df_clean = test_df[test_df[label_col].notna()].copy()\n",
    "scored_df = test_df_clean[['date', 'symbol']].copy()\n",
    "scored_df['ml_score'] = scores\n",
    "\n",
    "print(f\"✓ Generated ML scores for {len(scored_df):,} observations\")\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(scored_df['ml_score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get price panel (always use real close prices!)\n",
    "price_panel = df[['date', 'symbol', 'close']].copy()\n",
    "\n",
    "print(f\"Price panel: {len(price_panel):,} rows\")\n",
    "print(f\"Date range: {price_panel['date'].min()} to {price_panel['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct portfolio weights over time\n",
    "print(\"Constructing portfolio weights...\")\n",
    "print(f\"Using optimizer: {config['portfolio']['optimizer']}\")\n",
    "print(f\"Top K positions: {config['portfolio']['top_k']}\\n\")\n",
    "\n",
    "weights_history = []\n",
    "unique_dates = sorted(scored_df['date'].unique())\n",
    "\n",
    "# Limit to first 50 dates for demo (faster execution)\n",
    "MAX_DATES = 50\n",
    "rebalance_dates = unique_dates[:min(MAX_DATES, len(unique_dates))]\n",
    "\n",
    "print(f\"Constructing portfolio for {len(rebalance_dates)} rebalance dates...\")\n",
    "\n",
    "for i, date in enumerate(rebalance_dates):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Processing date {i+1}/{len(rebalance_dates)}: {date}\")\n",
    "    \n",
    "    day_scores = scored_df[scored_df['date'] == date]\n",
    "    weights = construct_portfolio(day_scores, price_panel, config)\n",
    "    \n",
    "    for symbol, weight in weights.items():\n",
    "        weights_history.append({\n",
    "            'date': date,\n",
    "            'symbol': symbol,\n",
    "            'weight': weight\n",
    "        })\n",
    "\n",
    "weights_df = pd.DataFrame(weights_history)\n",
    "print(f\"\\n✓ Portfolio weights constructed\")\n",
    "print(f\"  Total weight records: {len(weights_df):,}\")\n",
    "print(f\"  Average positions per rebalance: {len(weights_df) / len(rebalance_dates):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine portfolio weights\n",
    "print(\"Sample portfolio weights:\")\n",
    "print(weights_df.head(20))\n",
    "\n",
    "# Weight statistics\n",
    "print(f\"\\nWeight statistics:\")\n",
    "print(weights_df['weight'].describe())\n",
    "\n",
    "# Check constraints\n",
    "weights_sum = weights_df.groupby('date')['weight'].sum()\n",
    "print(f\"\\nPortfolio weight sum per date:\")\n",
    "print(f\"  Mean: {weights_sum.mean():.4f}\")\n",
    "print(f\"  Min:  {weights_sum.min():.4f}\")\n",
    "print(f\"  Max:  {weights_sum.max():.4f}\")\n",
    "print(f\"\\n✓ Weights sum to ~1.0 (constraint satisfied)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtest\n",
    "print(\"Running backtest...\")\n",
    "print(f\"Cost assumptions:\")\n",
    "print(f\"  Commission: {config['portfolio']['costs_bps']} bps\")\n",
    "print(f\"  Slippage:   {config['portfolio']['slippage_bps']} bps\\n\")\n",
    "\n",
    "backtester = VectorizedBacktester(config)\n",
    "results = backtester.run(weights_df, price_panel)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BACKTEST RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTotal Return:        {results['metrics']['total_return']:.2%}\")\n",
    "print(f\"Annualized Return:   {results['metrics'].get('annual_return', 0):.2%}\")\n",
    "print(f\"Sharpe Ratio:        {results['metrics']['sharpe_ratio']:.2f}\")\n",
    "print(f\"Max Drawdown:        {results['metrics']['max_drawdown']:.2%}\")\n",
    "print(f\"Volatility:          {results['metrics'].get('volatility', 0):.2%}\")\n",
    "print(f\"Avg Turnover:        {results['metrics'].get('avg_turnover', 0):.2%}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize equity curve\n",
    "equity_curve = results['equity_curve']\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Equity curve\n",
    "axes[0].plot(equity_curve['date'], equity_curve['equity'], linewidth=2, label='Portfolio Equity')\n",
    "axes[0].set_title('Portfolio Equity Curve', fontsize=16, fontweight='bold')\n",
    "axes[0].set_ylabel('Equity ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Drawdown\n",
    "running_max = equity_curve['equity'].expanding().max()\n",
    "drawdown = (equity_curve['equity'] - running_max) / running_max * 100\n",
    "\n",
    "axes[1].fill_between(equity_curve['date'], drawdown, 0, alpha=0.5, color='red', label='Drawdown')\n",
    "axes[1].set_title('Drawdown', fontsize=16, fontweight='bold')\n",
    "axes[1].set_ylabel('Drawdown (%)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save backtest results\n",
    "import os\n",
    "os.makedirs('data/reports', exist_ok=True)\n",
    "equity_curve.to_csv('data/reports/equity_curve.csv', index=False)\n",
    "print(\"✓ Backtest results saved to data/reports/equity_curve.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Populate Dashboard\n",
    "\n",
    "Save results to database so the dashboard displays them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.database.models import init_database, PortfolioSnapshot\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize database\n",
    "engine, SessionLocal = init_database()\n",
    "db = SessionLocal()\n",
    "\n",
    "print(\"✓ Database connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate portfolio snapshots for dashboard\n",
    "print(\"Populating dashboard data...\")\n",
    "\n",
    "snapshots_to_add = []\n",
    "for _, row in equity_curve.iterrows():\n",
    "    snapshot = PortfolioSnapshot(\n",
    "        date=row['date'],\n",
    "        total_equity=row['equity'],\n",
    "        cash=0,  # Simplified - fully invested\n",
    "        positions_value=row['equity'],\n",
    "        daily_return=0,  # Can calculate if needed\n",
    "        daily_pnl=0\n",
    "    )\n",
    "    snapshots_to_add.append(snapshot)\n",
    "\n",
    "# Bulk insert\n",
    "db.bulk_save_objects(snapshots_to_add)\n",
    "db.commit()\n",
    "\n",
    "print(f\"✓ Added {len(snapshots_to_add)} portfolio snapshots to database\")\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ✅ Pipeline Complete!\n",
    "\n",
    "### What You Accomplished:\n",
    "1. ✓ Fetched OHLCV data for stocks\n",
    "2. ✓ Generated technical features and labels\n",
    "3. ✓ Trained ML model with evaluation metrics\n",
    "4. ✓ Constructed portfolio and ran backtest\n",
    "5. ✓ Populated dashboard database\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**1. View Results in Dashboard:**\n",
    "   - Go to http://localhost:8501 in your browser\n",
    "   - Refresh the page to see your data\n",
    "   - Navigate through Portfolio Overview, Signals, etc.\n",
    "\n",
    "**2. Experiment & Iterate:**\n",
    "   - Adjust `NUM_STOCKS` in Step 1 to test with more stocks\n",
    "   - Modify parameters in `config/config.yaml`\n",
    "   - Try different optimizers: `pypfopt`, `inverse_vol`, `equal_weight`\n",
    "   - Change `top_k` for number of positions\n",
    "\n",
    "**3. Production Automation:**\n",
    "   - Once satisfied, use `python run_pipeline.py` for automated runs\n",
    "   - Set up daily cron jobs for live operations\n",
    "   - Use `src/live/` modules for real-time trading\n",
    "\n",
    "**4. Further Analysis:**\n",
    "   - View MLflow UI: `mlflow ui --port 5000`\n",
    "   - Compare different model runs\n",
    "   - Analyze feature importance\n",
    "\n",
    "---\n",
    "\n",
    "### Development → Production Checklist\n",
    "- [ ] Test with larger universe (100+ stocks)\n",
    "- [ ] Validate PIT alignment for fundamentals\n",
    "- [ ] Optimize hyperparameters with Optuna\n",
    "- [ ] Paper trade with `dry_run=True`\n",
    "- [ ] Set up monitoring and alerts\n",
    "- [ ] Document your strategy parameters\n",
    "- [ ] Consider premium data providers\n",
    "\n",
    "**Great job! Your ML trading system is now operational.** 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "us-stock-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
