{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Trading System - Interactive Development Pipeline\n",
    "\n",
    "This notebook walks through the entire ML trading pipeline step-by-step.\n",
    "\n",
    "**Purpose**: Learn and experiment with each component during development phase.\n",
    "\n",
    "**Steps**:\n",
    "1. Data Ingestion (~5 min)\n",
    "2. Feature Engineering (~2 min)\n",
    "3. Model Training (~5 min)\n",
    "4. Backtesting (~2 min)\n",
    "5. Dashboard Population (~1 min)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config_path = os.path.join(project_root, 'config/config.yaml')\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"‚úì Configuration loaded\")\n",
    "print(f\"  - Modeling algorithm: {config['modeling']['algorithm']}\")\n",
    "print(f\"  - Portfolio top_k: {config['portfolio']['top_k']}\")\n",
    "print(f\"  - Optimizer: {config['portfolio']['optimizer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Data Ingestion\n",
    "\n",
    "Fetch OHLCV data for a subset of S&P 500 stocks.\n",
    "\n",
    "**Tip**: Start with 20-30 stocks for quick testing, expand later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.io.universe import load_sp500_constituents\n",
    "from src.io.ingest_ohlcv import OHLCVIngester\n",
    "\n",
    "# Load universe - START SMALL for testing\n",
    "NUM_STOCKS = 30  # Adjust this: 20-30 for testing, 100+ for production\n",
    "symbols = load_sp500_constituents()[:NUM_STOCKS]\n",
    "\n",
    "print(f\"Selected {len(symbols)} stocks for analysis\")\n",
    "print(f\"Sample symbols: {symbols[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch OHLCV data from yfinance\n",
    "ingester = OHLCVIngester()\n",
    "start_date = config['ingest']['start_date']  # From config.yaml\n",
    "\n",
    "print(f\"Fetching data from {start_date} to present...\")\n",
    "print(\"This may take 2-5 minutes depending on number of stocks...\\n\")\n",
    "\n",
    "data = ingester.fetch_ohlcv(symbols, start_date, None)\n",
    "\n",
    "print(f\"\\n‚úì Fetched data for {len(data)} symbols\")\n",
    "print(f\"  Total rows: {sum(len(df) for df in data.values()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to parquet format\n",
    "ingester.save_parquet(data)\n",
    "print(\"‚úì Data saved to data/parquet/\")\n",
    "\n",
    "# Explore the data\n",
    "sample_symbol = symbols[0]\n",
    "sample_df = data[sample_symbol]\n",
    "\n",
    "print(f\"\\nSample data for {sample_symbol}:\")\n",
    "print(sample_df.head())\n",
    "print(f\"\\nDate range: {sample_df['date'].min()} to {sample_df['date'].max()}\")\n",
    "print(f\"Total trading days: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample stock price\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Price chart\n",
    "axes[0].plot(sample_df['date'], sample_df['close'], linewidth=1.5)\n",
    "axes[0].set_title(f\"{sample_symbol} - Closing Price\", fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Price ($)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Volume chart\n",
    "axes[1].bar(sample_df['date'], sample_df['volume'], alpha=0.6)\n",
    "axes[1].set_title(f\"{sample_symbol} - Trading Volume\", fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Volume')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.io.ingest_fundamentals import FundamentalsIngester\n",
    "\n",
    "# Initialize fundamental data ingester\n",
    "fund_ingester = FundamentalsIngester(storage_path=\"data/fundamentals\")\n",
    "\n",
    "print(f\"Fetching fundamental data for {len(symbols)} symbols...\")\n",
    "print(\"This may take 2-3 minutes...\\n\")\n",
    "\n",
    "# Fetch fundamental data\n",
    "fundamental_data = fund_ingester.fetch_fundamentals(symbols)\n",
    "\n",
    "print(f\"\\n‚úì Fetched fundamental data for {len(fundamental_data)} symbols\")\n",
    "\n",
    "# Save to parquet\n",
    "fund_ingester.save_parquet(fundamental_data)\n",
    "print(\"‚úì Fundamental data saved to data/fundamentals/\")\n",
    "\n",
    "# Preview sample data\n",
    "if len(fundamental_data) > 0:\n",
    "    sample_symbol = list(fundamental_data.keys())[0]\n",
    "    sample_fund = fundamental_data[sample_symbol]\n",
    "    print(f\"\\nSample fundamental data for {sample_symbol}:\")\n",
    "    print(sample_fund.head())\n",
    "    print(f\"\\nColumns: {list(sample_fund.columns)}\")\n",
    "    print(f\"Date range: {sample_fund['date'].min()} to {sample_fund['date'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Fetch Fundamental Data\n",
    "\n",
    "Now let's fetch fundamental data (income statements, balance sheets, cash flow statements) for the same symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Feature Engineering\n",
    "\n",
    "Generate technical indicators and forward return labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.features.ta_features import create_technical_features\n",
    "from src.labeling.labels import generate_forward_returns\n",
    "\n",
    "# Load saved parquet data\n",
    "df = ingester.load_parquet(symbols)\n",
    "\n",
    "print(f\"Loaded {len(df):,} rows of OHLCV data\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "print(f\"Unique symbols: {df['symbol'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.5: Add Fundamental Features with PIT Alignment\n",
    "\n",
    "Now let's integrate fundamental features using Point-in-Time (PIT) alignment to prevent look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the module to get latest fixes\n",
    "import importlib\n",
    "import sys\n",
    "if 'src.features.fa_features' in sys.modules:\n",
    "    importlib.reload(sys.modules['src.features.fa_features'])\n",
    "    print(\"‚úì Reloaded fa_features module\")\n",
    "\n",
    "from src.features.fa_features import FundamentalFeatures\n",
    "\n",
    "print(\"Loading fundamental data...\")\n",
    "# Load all saved fundamental data\n",
    "fundamentals_df = fund_ingester.load_parquet(symbols)\n",
    "\n",
    "if fundamentals_df.empty:\n",
    "    print(\"‚ö†Ô∏è No fundamental data found. Run Step 1.5 first.\")\n",
    "else:\n",
    "    print(f\"‚úì Loaded {len(fundamentals_df)} fundamental records\")\n",
    "    print(f\"  Symbols: {fundamentals_df['symbol'].nunique()}\")\n",
    "    print(f\"  Date range: {fundamentals_df['date'].min()} to {fundamentals_df['date'].max()}\")\n",
    "    \n",
    "    # Initialize fundamental features with PIT alignment\n",
    "    fa_features = FundamentalFeatures(config)\n",
    "    \n",
    "    print(\"\\nComputing fundamental features with PIT alignment...\")\n",
    "    print(f\"  PIT constraints:\")\n",
    "    print(f\"    - Min lag: {fa_features.pit_min_lag_days} days\")\n",
    "    print(f\"    - Default publication lag: {fa_features.default_public_lag_days} days\")\n",
    "    print(f\"    - Earnings blackout: {fa_features.earnings_blackout_days} days\")\n",
    "    \n",
    "    # Store original column count\n",
    "    original_cols = len(df.columns)\n",
    "    \n",
    "    # Compute and align fundamental features\n",
    "    df = fa_features.compute_features(df, fundamentals_df)\n",
    "    \n",
    "    new_cols = len(df.columns) - original_cols\n",
    "    print(f\"\\n‚úì Fundamental features added: {new_cols} new columns\")\n",
    "    \n",
    "    # Show new fundamental feature columns\n",
    "    fund_feature_cols = [col for col in df.columns if any(x in col for x in ['ratio', 'roe', 'roa', 'debt', 'margin', 'qoq', 'quality', 'equity', 'revenue', 'income', 'cash_flow'])]\n",
    "    print(f\"\\nSample fundamental features:\")\n",
    "    for col in fund_feature_cols[:15]:\n",
    "        print(f\"  - {col}\")\n",
    "    if len(fund_feature_cols) > 15:\n",
    "        print(f\"  ... and {len(fund_feature_cols) - 15} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate technical features\n",
    "print(\"Generating technical features...\")\n",
    "df = create_technical_features(df, config)\n",
    "\n",
    "print(f\"\\n‚úì Technical features created\")\n",
    "print(f\"  Total columns now: {len(df.columns)}\")\n",
    "\n",
    "# Show feature columns\n",
    "feature_cols = [col for col in df.columns if col not in ['date', 'symbol', 'open', 'high', 'low', 'close', 'volume', 'adj_close']]\n",
    "print(f\"\\nGenerated features ({len(feature_cols)}):\")\n",
    "for col in feature_cols[:10]:\n",
    "    print(f\"  - {col}\")\n",
    "if len(feature_cols) > 10:\n",
    "    print(f\"  ... and {len(feature_cols) - 10} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate forward return labels\n",
    "print(\"Generating forward return labels...\")\n",
    "df = generate_forward_returns(df, config)\n",
    "\n",
    "label_col = f\"forward_return_{config['labels']['horizon']}d\"\n",
    "print(f\"\\n‚úì Labels created: {label_col}\")\n",
    "print(f\"\\nLabel statistics:\")\n",
    "print(df[label_col].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize label distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df[label_col].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0].set_title('Forward Returns Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel(f'{label_col} (%)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero return')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot by symbol (sample)\n",
    "sample_symbols = symbols[:10]\n",
    "sample_data = df[df['symbol'].isin(sample_symbols)]\n",
    "sample_data.boxplot(column=label_col, by='symbol', ax=axes[1], rot=45)\n",
    "axes[1].set_title('Forward Returns by Symbol (Sample)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Symbol')\n",
    "axes[1].set_ylabel(f'{label_col} (%)')\n",
    "plt.suptitle('')  # Remove default title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save features\n",
    "from src.io.storage import save_dataframe\n",
    "\n",
    "save_dataframe(df, 'data/features/all_features.parquet')\n",
    "print(\"‚úì Features saved to data/features/all_features.parquet\")\n",
    "\n",
    "# Check for missing values\n",
    "missing_pct = (df.isnull().sum() / len(df) * 100).sort_values(ascending=False)\n",
    "print(f\"\\nMissing values (top 10 columns with most nulls):\")\n",
    "print(missing_pct.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize fundamental features distribution\n",
    "try:\n",
    "    fundamentals_df\n",
    "    fund_feature_cols\n",
    "    can_visualize = not fundamentals_df.empty and len(fund_feature_cols) > 0\n",
    "except NameError:\n",
    "    can_visualize = False\n",
    "    print(\"‚ÑπÔ∏è No fundamental features to visualize\")\n",
    "\n",
    "if can_visualize:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Select key fundamental features\n",
    "    viz_features = []\n",
    "    for feat in ['pe_ratio_calc', 'roe_calc', 'debt_to_equity_calc', 'profit_margin']:\n",
    "        if feat in df.columns:\n",
    "            viz_features.append(feat)\n",
    "    \n",
    "    if len(viz_features) >= 4:\n",
    "        for idx, feat in enumerate(viz_features[:4]):\n",
    "            row = idx // 2\n",
    "            col = idx % 2\n",
    "            \n",
    "            data = df[feat].dropna()\n",
    "            if len(data) > 0:\n",
    "                axes[row, col].hist(data, bins=50, alpha=0.7, edgecolor='black')\n",
    "                axes[row, col].set_title(f'{feat} Distribution', fontsize=12, fontweight='bold')\n",
    "                axes[row, col].set_xlabel(feat)\n",
    "                axes[row, col].set_ylabel('Frequency')\n",
    "                axes[row, col].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add mean/median lines\n",
    "                mean_val = data.mean()\n",
    "                median_val = data.median()\n",
    "                axes[row, col].axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "                axes[row, col].axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.2f}')\n",
    "                axes[row, col].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FUNDAMENTAL FEATURES SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total fundamental features: {len(fund_feature_cols)}\")\n",
    "        print(f\"\\nCoverage (non-null ratio):\")\n",
    "        for feat in fund_feature_cols[:10]:\n",
    "            if feat in df.columns:\n",
    "                coverage = (df[feat].notna().sum() / len(df)) * 100\n",
    "                print(f\"  {feat}: {coverage:.1f}%\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Only {len(viz_features)} fundamental features found (need 4 for visualization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Fundamental Features Distribution\n",
    "\n",
    "Visualize the distribution of key fundamental features to understand the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification: Check PIT alignment\n",
    "try:\n",
    "    fundamentals_df\n",
    "    has_fundamentals = not fundamentals_df.empty\n",
    "except NameError:\n",
    "    has_fundamentals = False\n",
    "    print(\"‚ö†Ô∏è fundamentals_df not defined - skipping PIT verification\")\n",
    "\n",
    "if has_fundamentals and 'public_date' in df.columns:\n",
    "    print(\"=\"*70)\n",
    "    print(\"PIT ALIGNMENT VERIFICATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Check 1: Ensure all public_dates are before or equal to price dates\n",
    "    df_with_fund = df[df['public_date'].notna()].copy()\n",
    "    \n",
    "    if len(df_with_fund) > 0:\n",
    "        # Calculate days between public_date and price date\n",
    "        df_with_fund['days_after_publication'] = (df_with_fund['date'] - df_with_fund['public_date']).dt.days\n",
    "        \n",
    "        print(f\"\\n‚úì Fundamental data coverage: {len(df_with_fund):,} rows ({len(df_with_fund)/len(df)*100:.1f}%)\")\n",
    "        print(f\"\\nDays between publication and price date:\")\n",
    "        print(df_with_fund['days_after_publication'].describe())\n",
    "        \n",
    "        # Check for any violations (price date before public date)\n",
    "        violations = df_with_fund[df_with_fund['days_after_publication'] < 0]\n",
    "        if len(violations) > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è WARNING: Found {len(violations)} PIT violations!\")\n",
    "            print(\"These rows have price dates BEFORE fundamental publication dates.\")\n",
    "            print(violations[['date', 'symbol', 'public_date', 'days_after_publication']].head())\n",
    "        else:\n",
    "            print(f\"\\n‚úì No PIT violations detected\")\n",
    "        \n",
    "        # Check 2: Verify minimum lag constraint\n",
    "        try:\n",
    "            fa_features\n",
    "            min_lag_violations = df_with_fund[df_with_fund['days_after_publication'] < fa_features.pit_min_lag_days]\n",
    "            if len(min_lag_violations) > 0:\n",
    "                print(f\"\\n‚ö†Ô∏è WARNING: {len(min_lag_violations)} rows violate minimum lag ({fa_features.pit_min_lag_days} days)\")\n",
    "            else:\n",
    "                print(f\"\\n‚úì All rows respect minimum lag constraint ({fa_features.pit_min_lag_days} days)\")\n",
    "        except NameError:\n",
    "            pass\n",
    "        \n",
    "        # Sample verification\n",
    "        print(f\"\\nSample PIT-aligned data (5 random rows):\")\n",
    "        sample_cols = ['date', 'symbol', 'public_date', 'days_after_publication']\n",
    "        if 'pe_ratio_calc' in df_with_fund.columns:\n",
    "            sample_cols.append('pe_ratio_calc')\n",
    "        \n",
    "        sample_rows = df_with_fund.sample(min(5, len(df_with_fund)))[sample_cols]\n",
    "        for _, row in sample_rows.iterrows():\n",
    "            print(f\"  {row['symbol']} on {row['date'].date()}: data from {row['public_date'].date()} ({int(row['days_after_publication'])} days ago)\")\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "elif has_fundamentals:\n",
    "    print(\"‚ö†Ô∏è Cannot verify PIT alignment - public_date column not found\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No fundamental data loaded - PIT verification skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification: PIT Alignment Check\n",
    "\n",
    "Verify that fundamental data is properly aligned and no look-ahead bias exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Model Training\n",
    "\n",
    "Train ML model to predict forward returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ml.dataset import MLDataset, create_time_based_split\n",
    "from src.ml.train import ModelTrainer\n",
    "\n",
    "# Prepare dataset\n",
    "label_col = f\"forward_return_{config['labels']['horizon']}d\"\n",
    "dataset = MLDataset(label_col=label_col)\n",
    "\n",
    "print(f\"Preparing ML dataset with label: {label_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based train/test split (crucial for time-series)\n",
    "embargo_days = config['modeling'].get('embargo_days', 5)\n",
    "train_df, test_df = create_time_based_split(df, test_size=0.2, embargo_days=embargo_days)\n",
    "\n",
    "print(f\"‚úì Data split with {embargo_days}-day embargo\")\n",
    "print(f\"\\nTrain set:\")\n",
    "print(f\"  Rows: {len(train_df):,}\")\n",
    "print(f\"  Date range: {train_df['date'].min()} to {train_df['date'].max()}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Rows: {len(test_df):,}\")\n",
    "print(f\"  Date range: {test_df['date'].min()} to {test_df['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with MLflow tracking\n",
    "trainer = ModelTrainer(config)\n",
    "\n",
    "print(f\"Training {config['modeling']['algorithm']} model...\")\n",
    "print(\"This may take 3-5 minutes...\\n\")\n",
    "\n",
    "trainer.train_with_mlflow(\n",
    "    X_train, y_train, \n",
    "    X_test, y_test, \n",
    "    run_name=\"interactive_notebook_run\"\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Model training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and labels\n",
    "X_train, y_train = dataset.prepare(train_df, auto_select_features=True)\n",
    "X_test, y_test = dataset.prepare(test_df, auto_select_features=False)\n",
    "\n",
    "print(f\"‚úì Features prepared\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Samples: {len(X_train):,}\")\n",
    "print(f\"  Features: {len(X_train.columns)}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Samples: {len(X_test):,}\")\n",
    "print(f\"\\nSelected features:\")\n",
    "for feat in X_train.columns[:15]:\n",
    "    print(f\"  - {feat}\")\n",
    "if len(X_train.columns) > 15:\n",
    "    print(f\"  ... and {len(X_train.columns) - 15} more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "metrics = trainer.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\n=\" * 50)\n",
    "print(\"MODEL EVALUATION METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nInformation Coefficient (IC):      {metrics['ic']:.4f}\")\n",
    "print(f\"Rank IC:                            {metrics['rank_ic']:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE):           {metrics['mse']:.6f}\")\n",
    "print(f\"R¬≤ Score:                           {metrics.get('r2', 0):.4f}\")\n",
    "\n",
    "# IC interpretation guide\n",
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"IC Interpretation Guide:\")\n",
    "print(\"  |IC| > 0.05: Good predictive power\")\n",
    "print(\"  |IC| > 0.10: Excellent predictive power\")\n",
    "print(\"  IC > 0:     Positive correlation (correct direction)\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions vs actuals\n",
    "predictions = trainer.predict(X_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scatter plot\n",
    "axes[0].scatter(y_test, predictions, alpha=0.3, s=10)\n",
    "axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect prediction')\n",
    "axes[0].set_xlabel('Actual Forward Returns')\n",
    "axes[0].set_ylabel('Predicted Forward Returns')\n",
    "axes[0].set_title('Predictions vs Actuals', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals\n",
    "residuals = y_test - predictions\n",
    "axes[1].hist(residuals, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero error')\n",
    "axes[1].set_xlabel('Prediction Error')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Prediction Residuals', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import os\n",
    "model_path = 'data/models/latest_model.pkl'\n",
    "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "trainer.save_model(model_path)\n",
    "print(f\"‚úì Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Portfolio Construction & Backtesting\n",
    "\n",
    "Use ML scores to construct portfolio and backtest with realistic costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.portfolio.construct import construct_portfolio\n",
    "from src.backtest.bt_engine import VectorizedBacktester\n",
    "\n",
    "# Generate scores for test period\n",
    "X_test_full, _ = dataset.prepare(test_df, auto_select_features=False)\n",
    "scores = trainer.predict(X_test_full)\n",
    "\n",
    "# Filter test_df to match X_test_full (remove rows with missing labels)\n",
    "test_df_clean = test_df[test_df[label_col].notna()].copy()\n",
    "scored_df = test_df_clean[['date', 'symbol']].copy()\n",
    "scored_df['ml_score'] = scores\n",
    "\n",
    "print(f\"‚úì Generated ML scores for {len(scored_df):,} observations\")\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(scored_df['ml_score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get price panel (always use real close prices!)\n",
    "price_panel = df[['date', 'symbol', 'close']].copy()\n",
    "\n",
    "print(f\"Price panel: {len(price_panel):,} rows\")\n",
    "print(f\"Date range: {price_panel['date'].min()} to {price_panel['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct portfolio weights over time\n",
    "print(\"Constructing portfolio weights...\")\n",
    "print(f\"Using optimizer: {config['portfolio']['optimizer']}\")\n",
    "print(f\"Top K positions: {config['portfolio']['top_k']}\\n\")\n",
    "\n",
    "weights_history = []\n",
    "unique_dates = sorted(scored_df['date'].unique())\n",
    "\n",
    "# Limit to first 50 dates for demo (faster execution)\n",
    "MAX_DATES = 50\n",
    "rebalance_dates = unique_dates[:min(MAX_DATES, len(unique_dates))]\n",
    "\n",
    "print(f\"Constructing portfolio for {len(rebalance_dates)} rebalance dates...\")\n",
    "\n",
    "for i, date in enumerate(rebalance_dates):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Processing date {i+1}/{len(rebalance_dates)}: {date}\")\n",
    "    \n",
    "    day_scores = scored_df[scored_df['date'] == date]\n",
    "    weights = construct_portfolio(day_scores, price_panel, config)\n",
    "    \n",
    "    for symbol, weight in weights.items():\n",
    "        weights_history.append({\n",
    "            'date': date,\n",
    "            'symbol': symbol,\n",
    "            'weight': weight\n",
    "        })\n",
    "\n",
    "weights_df = pd.DataFrame(weights_history)\n",
    "print(f\"\\n‚úì Portfolio weights constructed\")\n",
    "print(f\"  Total weight records: {len(weights_df):,}\")\n",
    "print(f\"  Average positions per rebalance: {len(weights_df) / len(rebalance_dates):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine portfolio weights\n",
    "print(\"Sample portfolio weights:\")\n",
    "print(weights_df.head(20))\n",
    "\n",
    "# Weight statistics\n",
    "print(f\"\\nWeight statistics:\")\n",
    "print(weights_df['weight'].describe())\n",
    "\n",
    "# Check constraints\n",
    "weights_sum = weights_df.groupby('date')['weight'].sum()\n",
    "print(f\"\\nPortfolio weight sum per date:\")\n",
    "print(f\"  Mean: {weights_sum.mean():.4f}\")\n",
    "print(f\"  Min:  {weights_sum.min():.4f}\")\n",
    "print(f\"  Max:  {weights_sum.max():.4f}\")\n",
    "print(f\"\\n‚úì Weights sum to ~1.0 (constraint satisfied)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run backtest\n",
    "print(\"Running backtest...\")\n",
    "print(f\"Cost assumptions:\")\n",
    "print(f\"  Commission: {config['portfolio']['costs_bps']} bps\")\n",
    "print(f\"  Slippage:   {config['portfolio']['slippage_bps']} bps\\n\")\n",
    "\n",
    "backtester = VectorizedBacktester(config)\n",
    "results = backtester.run(weights_df, price_panel)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"BACKTEST RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"\\nTotal Return:        {results['metrics']['total_return']:.2%}\")\n",
    "print(f\"Annualized Return:   {results['metrics'].get('annual_return', 0):.2%}\")\n",
    "print(f\"Sharpe Ratio:        {results['metrics']['sharpe_ratio']:.2f}\")\n",
    "print(f\"Max Drawdown:        {results['metrics']['max_drawdown']:.2%}\")\n",
    "print(f\"Volatility:          {results['metrics'].get('volatility', 0):.2%}\")\n",
    "print(f\"Avg Turnover:        {results['metrics'].get('avg_turnover', 0):.2%}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize equity curve\n",
    "equity_curve = results['equity_curve']\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Equity curve\n",
    "axes[0].plot(equity_curve['date'], equity_curve['equity'], linewidth=2, label='Portfolio Equity')\n",
    "axes[0].set_title('Portfolio Equity Curve', fontsize=16, fontweight='bold')\n",
    "axes[0].set_ylabel('Equity ($)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Drawdown\n",
    "running_max = equity_curve['equity'].expanding().max()\n",
    "drawdown = (equity_curve['equity'] - running_max) / running_max * 100\n",
    "\n",
    "axes[1].fill_between(equity_curve['date'], drawdown, 0, alpha=0.5, color='red', label='Drawdown')\n",
    "axes[1].set_title('Drawdown', fontsize=16, fontweight='bold')\n",
    "axes[1].set_ylabel('Drawdown (%)')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save backtest results\n",
    "import os\n",
    "os.makedirs('data/reports', exist_ok=True)\n",
    "equity_curve.to_csv('data/reports/equity_curve.csv', index=False)\n",
    "print(\"‚úì Backtest results saved to data/reports/equity_curve.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: Populate Dashboard\n",
    "\n",
    "Save results to database so the dashboard displays them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.database.models import init_database, PortfolioSnapshot\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize database\n",
    "engine, SessionLocal = init_database()\n",
    "db = SessionLocal()\n",
    "\n",
    "print(\"‚úì Database connection established\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate portfolio snapshots for dashboard\n",
    "print(\"Populating dashboard data...\")\n",
    "\n",
    "snapshots_to_add = []\n",
    "for _, row in equity_curve.iterrows():\n",
    "    snapshot = PortfolioSnapshot(\n",
    "        date=row['date'],\n",
    "        total_equity=row['equity'],\n",
    "        cash=0,  # Simplified - fully invested\n",
    "        positions_value=row['equity'],\n",
    "        daily_return=0,  # Can calculate if needed\n",
    "        daily_pnl=0\n",
    "    )\n",
    "    snapshots_to_add.append(snapshot)\n",
    "\n",
    "# Bulk insert\n",
    "db.bulk_save_objects(snapshots_to_add)\n",
    "db.commit()\n",
    "\n",
    "print(f\"‚úì Added {len(snapshots_to_add)} portfolio snapshots to database\")\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ Pipeline Complete!\n",
    "\n",
    "### What You Accomplished:\n",
    "1. ‚úì Fetched OHLCV data for stocks\n",
    "2. ‚úì Generated technical features and labels\n",
    "3. ‚úì Trained ML model with evaluation metrics\n",
    "4. ‚úì Constructed portfolio and ran backtest\n",
    "5. ‚úì Populated dashboard database\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "**1. View Results in Dashboard:**\n",
    "   - Go to http://localhost:8501 in your browser\n",
    "   - Refresh the page to see your data\n",
    "   - Navigate through Portfolio Overview, Signals, etc.\n",
    "\n",
    "**2. Experiment & Iterate:**\n",
    "   - Adjust `NUM_STOCKS` in Step 1 to test with more stocks\n",
    "   - Modify parameters in `config/config.yaml`\n",
    "   - Try different optimizers: `pypfopt`, `inverse_vol`, `equal_weight`\n",
    "   - Change `top_k` for number of positions\n",
    "\n",
    "**3. Production Automation:**\n",
    "   - Once satisfied, use `python run_pipeline.py` for automated runs\n",
    "   - Set up daily cron jobs for live operations\n",
    "   - Use `src/live/` modules for real-time trading\n",
    "\n",
    "**4. Further Analysis:**\n",
    "   - View MLflow UI: `mlflow ui --port 5000`\n",
    "   - Compare different model runs\n",
    "   - Analyze feature importance\n",
    "\n",
    "---\n",
    "\n",
    "### Development ‚Üí Production Checklist\n",
    "- [ ] Test with larger universe (100+ stocks)\n",
    "- [ ] Validate PIT alignment for fundamentals\n",
    "- [ ] Optimize hyperparameters with Optuna\n",
    "- [ ] Paper trade with `dry_run=True`\n",
    "- [ ] Set up monitoring and alerts\n",
    "- [ ] Document your strategy parameters\n",
    "- [ ] Consider premium data providers\n",
    "\n",
    "**Great job! Your ML trading system is now operational.** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "us-stock-app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
